{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fbe5cd-98f6-4f87-9fe2-5149d0b4597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install JAEN -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248b6739-6a6e-4ada-a77c-a5f3b346ba00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from JAEN.utils import plot_training_results\n",
    "\n",
    "# device 설정 (GPU가 사용 가능하면 GPU로, 그렇지 않으면 CPU 사용)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d58c3f-ff3d-47d1-8d92-5c8b87ccc78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000019C9C5AC310>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000019C9C5AC670>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# JAEN 모듈 불러오기\n",
    "from JAEN.datasets import load_titanic\n",
    "\n",
    "# 데이터 파일이 저장된 경로 찾기\n",
    "jaen_path = os.path.dirname(load_titanic.__code__.co_filename)\n",
    "train_path = os.path.join(jaen_path, \"data\", \"00\", \"train_loader.pt\")\n",
    "test_path = os.path.join(jaen_path, \"data\", \"00\", \"test_loader.pt\")\n",
    "\n",
    "# torch.load()를 직접 실행 (weights_only=False 설정)\n",
    "train_loader = torch.load(train_path, weights_only=False)\n",
    "test_loader = torch.load(test_path, weights_only=False)\n",
    "\n",
    "# 데이터 확인\n",
    "print(train_loader)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10808317-1de6-4e83-9516-5f8897092ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(7, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = model.to(device)\n",
    "summary(model, (32, 7))\n",
    "\n",
    "# 32 → 한 번에 모델에 입력하는 샘플 개수 (Batch Size)\n",
    "# 7 → 각 샘플의 특성(feature) 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca9d01-e692-43c7-8885-a3d9d55d5b6a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080423e-a6fb-4518-8988-455a9085ba1e",
   "metadata": {},
   "source": [
    "## nn.Module 기반 신경망 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9c6d06-44ef-44f1-85d1-6c3c321f3b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DNN                                      [32, 1]                   --\n",
       "├─Linear: 1-1                            [32, 32]                  256\n",
       "├─ReLU: 1-2                              [32, 32]                  --\n",
       "├─Linear: 1-3                            [32, 1]                   33\n",
       "├─Sigmoid: 1-4                           [32, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 289\n",
       "Trainable params: 289\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(7, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = DNN().to(device)\n",
    "summary(model, (32, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b3d52-fc67-442d-86c8-7514ca326c0d",
   "metadata": {},
   "source": [
    "## 모델 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fdf9ea-8a55-45f0-a069-9876de89001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01df457-0b09-4ada-b4aa-82ce8baad202",
   "metadata": {},
   "source": [
    "#### 모델 학습 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d603db10-dd33-445c-8e3d-7af6a8d94f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
    "\n",
    "    for data, labels in train_loader: # 미니 배치 별 파라미터 업데이트 수행\n",
    "        data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
    "\n",
    "        # 순전파\n",
    "        outputs = model(data)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 누적\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267ee96-d552-4ebe-9c22-dd0246d04fab",
   "metadata": {},
   "source": [
    "#### 모델 평가 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ead62d3-69c9-40a9-9e1a-7bd35932d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "\n",
    "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
    "\n",
    "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
    "        for data, labels in test_loader: # 미니 배치 별 손실 계산\n",
    "            data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
    "\n",
    "            # 순전파\n",
    "            outputs = model(data)\n",
    "\n",
    "            # 손실 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 손실 누적\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
    "    return running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70061b60-5008-4c96-ba4f-ba4f645b1945",
   "metadata": {},
   "source": [
    "#### 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5710544-f18c-4718-b746-e3eb38c8c5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DNN                                      [32, 1]                   --\n",
       "├─Linear: 1-1                            [32, 64]                  512\n",
       "├─BatchNorm1d: 1-2                       [32, 64]                  128\n",
       "├─ReLU: 1-3                              [32, 64]                  --\n",
       "├─Dropout: 1-4                           [32, 64]                  --\n",
       "├─Linear: 1-5                            [32, 32]                  2,080\n",
       "├─BatchNorm1d: 1-6                       [32, 32]                  64\n",
       "├─ReLU: 1-7                              [32, 32]                  --\n",
       "├─Linear: 1-8                            [32, 16]                  528\n",
       "├─BatchNorm1d: 1-9                       [32, 16]                  32\n",
       "├─ReLU: 1-10                             [32, 16]                  --\n",
       "├─Linear: 1-11                           [32, 1]                   17\n",
       "├─Sigmoid: 1-12                          [32, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 3,361\n",
       "Trainable params: 3,361\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(7, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DNN().to(device)\n",
    "summary(model, input_size = (32, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef67877-fdf7-4432-a935-fc9303e16338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3ea3f2-4d0d-4f41-8b0b-0a96db0d0e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss : 0.6935758279717487 Test Loss : 0.6815032462279002\n",
      "Epoch 2 Train Loss : 0.6974595059519229 Test Loss : 0.6970425049463908\n",
      "Epoch 3 Train Loss : 0.6964311107345249 Test Loss : 0.6977594395478567\n",
      "Epoch 4 Train Loss : 0.7055266940075419 Test Loss : 0.6992875138918558\n",
      "Epoch 5 Train Loss : 0.6923974285955015 Test Loss : 0.6998989482720693\n",
      "Epoch 6 Train Loss : 0.7016270497570867 Test Loss : 0.6952687601248423\n",
      "Epoch 7 Train Loss : 0.695244400397591 Test Loss : 0.7043977777163187\n",
      "Epoch 8 Train Loss : 0.7001471778620845 Test Loss : 0.7060245970884959\n",
      "Epoch 9 Train Loss : 0.6988425228906714 Test Loss : 0.7010207573572794\n",
      "Epoch 10 Train Loss : 0.6998503804206848 Test Loss : 0.7060646017392477\n",
      "Epoch 11 Train Loss : 0.6945584442304529 Test Loss : 0.7065034906069437\n",
      "Epoch 12 Train Loss : 0.6967361569404602 Test Loss : 0.7019996146361033\n",
      "Epoch 13 Train Loss : 0.6895307483880416 Test Loss : 0.7011136809984843\n",
      "Epoch 14 Train Loss : 0.6951934928479402 Test Loss : 0.6976640721162161\n",
      "Epoch 15 Train Loss : 0.6906086180521094 Test Loss : 0.699569414059321\n",
      "Epoch 16 Train Loss : 0.7067799982817277 Test Loss : 0.6982140640417734\n",
      "Epoch 17 Train Loss : 0.7011093497276306 Test Loss : 0.7008979121843973\n",
      "Epoch 18 Train Loss : 0.6989645413730455 Test Loss : 0.6965772112210592\n",
      "Epoch 19 Train Loss : 0.6989636628524117 Test Loss : 0.7060025036334991\n",
      "Epoch 20 Train Loss : 0.7092924895493881 Test Loss : 0.700064996878306\n",
      "Epoch 21 Train Loss : 0.7038164864415708 Test Loss : 0.7033412257830302\n",
      "Epoch 22 Train Loss : 0.704385853332022 Test Loss : 0.7071262001991272\n",
      "Epoch 23 Train Loss : 0.6954711001852284 Test Loss : 0.7022288044293722\n",
      "Epoch 24 Train Loss : 0.6944036043208578 Test Loss : 0.7007047931353251\n",
      "Epoch 25 Train Loss : 0.6927497179611869 Test Loss : 0.7069672346115112\n",
      "Epoch 26 Train Loss : 0.7038192749023438 Test Loss : 0.6992035905520121\n",
      "Epoch 27 Train Loss : 0.6948767563571101 Test Loss : 0.7037968933582306\n",
      "Epoch 28 Train Loss : 0.7028545348540597 Test Loss : 0.7007530033588409\n",
      "Epoch 29 Train Loss : 0.7016316082166589 Test Loss : 0.7063625355561575\n",
      "Epoch 30 Train Loss : 0.6953849533329839 Test Loss : 0.7052468061447144\n",
      "Epoch 31 Train Loss : 0.6929940633151842 Test Loss : 0.7092759907245636\n",
      "Epoch 32 Train Loss : 0.6984616569850756 Test Loss : 0.6964693864186605\n",
      "Epoch 33 Train Loss : 0.7048561987669572 Test Loss : 0.7011236945788065\n",
      "Epoch 34 Train Loss : 0.6966024300326472 Test Loss : 0.7046521107355753\n",
      "Epoch 35 Train Loss : 0.7056490597517594 Test Loss : 0.6994076371192932\n",
      "Epoch 36 Train Loss : 0.7082556615705076 Test Loss : 0.7039497196674347\n",
      "Epoch 37 Train Loss : 0.6982486481251924 Test Loss : 0.7069534063339233\n",
      "Epoch 38 Train Loss : 0.6955621994060018 Test Loss : 0.7094615201155344\n",
      "Epoch 39 Train Loss : 0.71006236905637 Test Loss : 0.7069819370905558\n",
      "Epoch 40 Train Loss : 0.7056442991546963 Test Loss : 0.7062377035617828\n",
      "Epoch 41 Train Loss : 0.6967429855595464 Test Loss : 0.7043242951234182\n",
      "Epoch 42 Train Loss : 0.7015439893888391 Test Loss : 0.7095017929871877\n",
      "Epoch 43 Train Loss : 0.7069153345149496 Test Loss : 0.7112071414788564\n",
      "Epoch 44 Train Loss : 0.6989189982414246 Test Loss : 0.7076834042867025\n",
      "Epoch 45 Train Loss : 0.6992094542669214 Test Loss : 0.7017743488152822\n",
      "Epoch 46 Train Loss : 0.6953504655672156 Test Loss : 0.7084631125132242\n",
      "Epoch 47 Train Loss : 0.7013584712277288 Test Loss : 0.7005017499128977\n",
      "Epoch 48 Train Loss : 0.6971695008485214 Test Loss : 0.7026464939117432\n",
      "Epoch 49 Train Loss : 0.6936184478842694 Test Loss : 0.707902729511261\n",
      "Epoch 50 Train Loss : 0.6938692849615345 Test Loss : 0.6966775357723236\n",
      "Epoch 51 Train Loss : 0.7032497525215149 Test Loss : 0.7045268813769022\n",
      "Epoch 52 Train Loss : 0.698901531488999 Test Loss : 0.7006903986136118\n",
      "Epoch 53 Train Loss : 0.6972945306612097 Test Loss : 0.7076422174771627\n",
      "Epoch 54 Train Loss : 0.6954744872839554 Test Loss : 0.7020906309286753\n",
      "Epoch 55 Train Loss : 0.7035993804102358 Test Loss : 0.7025140623251597\n",
      "Epoch 56 Train Loss : 0.7013179478438004 Test Loss : 0.7050839960575104\n",
      "Epoch 57 Train Loss : 0.6957655652709629 Test Loss : 0.6998296280701956\n",
      "Epoch 58 Train Loss : 0.6981633875681006 Test Loss : 0.703878919283549\n",
      "Epoch 59 Train Loss : 0.6963724234829778 Test Loss : 0.7022623419761658\n",
      "Epoch 60 Train Loss : 0.6922539809475774 Test Loss : 0.7070151666800181\n",
      "Epoch 61 Train Loss : 0.6985542126323866 Test Loss : 0.7031935155391693\n",
      "Epoch 62 Train Loss : 0.6980454662571782 Test Loss : 0.7026386857032776\n",
      "Epoch 63 Train Loss : 0.6985493820646534 Test Loss : 0.7008902529875437\n",
      "Epoch 64 Train Loss : 0.6973481463349384 Test Loss : 0.704787939786911\n",
      "Epoch 65 Train Loss : 0.7003074029217595 Test Loss : 0.7027513980865479\n",
      "Epoch 66 Train Loss : 0.6983435801837755 Test Loss : 0.7083330055077871\n",
      "Epoch 67 Train Loss : 0.6932004819745603 Test Loss : 0.6977084378401438\n",
      "Epoch 68 Train Loss : 0.7016410542571027 Test Loss : 0.70215372244517\n",
      "Epoch 69 Train Loss : 0.7002316195031871 Test Loss : 0.7036477526028951\n",
      "Epoch 70 Train Loss : 0.7003752926121587 Test Loss : 0.7077640692392985\n",
      "Epoch 71 Train Loss : 0.6954567121422809 Test Loss : 0.7046288053194681\n",
      "Epoch 72 Train Loss : 0.6963538708894149 Test Loss : 0.7004115382830302\n",
      "Epoch 73 Train Loss : 0.6949245281841444 Test Loss : 0.7064618766307831\n",
      "Epoch 74 Train Loss : 0.6985842637393785 Test Loss : 0.7026493847370148\n",
      "Epoch 75 Train Loss : 0.6940083400062893 Test Loss : 0.704303373893102\n",
      "Epoch 76 Train Loss : 0.7005962418473285 Test Loss : 0.7059337894121805\n",
      "Epoch 77 Train Loss : 0.7036359750706217 Test Loss : 0.7033638854821523\n",
      "Epoch 78 Train Loss : 0.7073972562085027 Test Loss : 0.7098899980386099\n",
      "Epoch 79 Train Loss : 0.6966869235038757 Test Loss : 0.7041473885377248\n",
      "Epoch 80 Train Loss : 0.7067150862320609 Test Loss : 0.7097331782182058\n",
      "Epoch 81 Train Loss : 0.698094375755476 Test Loss : 0.70417919754982\n",
      "Epoch 82 Train Loss : 0.7003814085670139 Test Loss : 0.7043028573195139\n",
      "Epoch 83 Train Loss : 0.6993060941281526 Test Loss : 0.7048554221789042\n",
      "Epoch 84 Train Loss : 0.706189645373303 Test Loss : 0.7041167616844177\n",
      "Epoch 85 Train Loss : 0.6962668040524358 Test Loss : 0.7039384047190348\n",
      "Epoch 86 Train Loss : 0.7021928144537884 Test Loss : 0.6998316943645477\n",
      "Epoch 87 Train Loss : 0.6958722964577053 Test Loss : 0.7012760043144226\n",
      "Epoch 88 Train Loss : 0.6999590837437174 Test Loss : 0.7038273711999258\n",
      "Epoch 89 Train Loss : 0.7031214962834897 Test Loss : 0.7043455839157104\n",
      "Epoch 90 Train Loss : 0.7067057023877683 Test Loss : 0.697010338306427\n",
      "Epoch 91 Train Loss : 0.6952958573465762 Test Loss : 0.7015646894772848\n",
      "Epoch 92 Train Loss : 0.6957357033439304 Test Loss : 0.6991448601086935\n",
      "Epoch 93 Train Loss : 0.6988359145496202 Test Loss : 0.6989197234312693\n",
      "Epoch 94 Train Loss : 0.6937312172806781 Test Loss : 0.7023084859053293\n",
      "Epoch 95 Train Loss : 0.6874725637228593 Test Loss : 0.7044952114423116\n",
      "Epoch 96 Train Loss : 0.702612068342126 Test Loss : 0.7014040847619375\n",
      "Epoch 97 Train Loss : 0.6933053524597831 Test Loss : 0.703790009021759\n",
      "Epoch 98 Train Loss : 0.6932689728944198 Test Loss : 0.7018483976523081\n",
      "Epoch 99 Train Loss : 0.7079016203465669 Test Loss : 0.7009837925434113\n",
      "Epoch 100 Train Loss : 0.6880057868750199 Test Loss : 0.6992026368776957\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# 학습 횟수 만큼 반복\n",
    "for epoch in range(100):\n",
    "\n",
    "    # 모델 학습(학습데이터)\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # 모델 평가 (평가데이터)\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc33e7d-3006-48dc-b350-7e9ecfca4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JAEN.utils import plot_training_results\n",
    "plot_training_results(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bdf66d-68f1-429c-a864-6b286b654132",
   "metadata": {},
   "source": [
    "#### 드랍아웃을 포함한 모델 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a7016-e5c9-44d7-b4d0-016629ae550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(7, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.1)  # 10%의 드랍아웃 적용\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = DNN().to(device)\n",
    "summary(model, (32, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249305b-49bd-480c-8e11-fb7826d3fe8c",
   "metadata": {},
   "source": [
    "#### Batch Normalization 및 Droupout 포함 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e56a5-2a41-47db-a578-03781f7b655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(7, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DNN().to(device)\n",
    "summary(model, input_size=(32, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b208a6-95be-4178-a7ef-fa96b5076e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
